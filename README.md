# The smallest Large Language Model (LLM) hardware device in the world
Our ongoing project develops a tiny device for efficient large language models (LLMs) inference optimized for low-power, embedded SoCs. It operates independently on minimal battery power for up to two days, providing intelligent, interactive LLM inference without internet connectivity. Enhanced by NPU offloading, GPU scheduling, and advanced optimization techniques, our device integrates language, voice, and sensor data for seamless human-device interaction. 

# Existing LLM inference on small or tiny devices meet three main challenges:
## Challenges:
1. **Hardware Constraints and Power Management:** limited CPU performance, small  memory, lack of powerful GPU and short battery life. 
2. **Model Efficiency and Size:** Developing LLMs that are both small enough to fit on low-power devices and efficient enough to perform at near-desktop levels is technically challenging.
3. **Multi-modality Processing:** Integrating and processing multiple types of inputs: text, audio, code and sensor data.
![image](https://github.com/JimmyLi-Network/tiny_LLM_device/assets/28814261/5ea6d580-b756-47f5-ae07-563a9a4b5fb9)
