# The smallest Large Language Model (LLM) hardware device in the world
Large language models (LLMs) have revolutionized a wide range of applications, from natural language processing to complex tasks in robotics and autonomous driving. Additionally, the significance of deploying LLMs on-device has surged recently. Executing LLMs on edge devices offers not only decreased latency and enhanced user experience but also supports heightened user privacy by enabling local data processing.

# Our Motivation
Our ongoing project develops a tiny device for efficient large language models (LLMs) inference optimized for low-power, embedded SoCs. It operates independently on minimal battery power for up to two days, providing intelligent, interactive LLM inference without internet connectivity. Enhanced by NPU offloading, GPU scheduling, and advanced optimization techniques, our device integrates language, voice, and sensor data for seamless human-device interaction. 

# Existing LLM inference on small or tiny devices meet three main challenges:
## Challenges:
1. **Hardware Constraints and Power Management:** limited CPU performance, small  memory, lack of powerful GPU and short battery life. 
2. **Model Efficiency and Size:** Developing LLMs that are both small enough to fit on low-power devices and efficient enough to perform at near-desktop levels is technically challenging.
3. **Multi-modality Processing:** Integrating and processing multiple types of inputs: text, audio, code and sensor data.
